[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyis of Web Browsing Data",
    "section": "",
    "text": "This guide walks readers through code implementing the analytical steps described in Clemm von Hohenberg, Stier, et al. (2024) “Analysis of Web Browsing Data: A Guide.” It is a more extended, and periodically updated, version of the Supplementary Materials (Part B) published with the paper. We advise readers to read the paper for more background regarding coding decisions.\nWe provide code examples in R and SQL. Our first thread of R code follows the tidy style. Our second thread of R coded applies the webtrackR (Schoch et al. 2023) package built specifically for web browsing data. Note that since the browsing data set is large, some of the examples may take a while to run in R.\n\n\n\n\nR. To run the R code, you need to install and load the following packages.\n\n# install.packages(\"tidyverse\")\n# install.packages(\"adaR\")\n## install.packages(\"devtools\")\n# devtools::install_github(\"schochastics/webtrackR\")\n# install.packages(\"httr2\")\n# install.packages(\"rvest\")\n\nlibrary(tidyverse)\nlibrary(adaR)\nlibrary(webtrackR)\nlibrary(httr2)\nlibrary(rvest)\n\nThe computational environment in which all the below R code was run is the following:\n\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rvest_1.0.3          httr2_0.2.3          webtrackR_0.1.0.9000\n [4] adaR_0.3.0           lubridate_1.9.2      forcats_1.0.0       \n [7] stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2         \n[10] readr_2.1.4          tidyr_1.3.0          tibble_3.2.1        \n[13] ggplot2_3.4.4        tidyverse_2.0.0     \n\n\nSQL. All SQL code was run on AWS Athena, but should work with any SQL platform that understands the Presto dialect.\n\n\n\nTo run the code, you need, first, a toy browsing data set from our OSF repository. It is based on a real data set collected from a US sample in 2019, but reduced to fewer participants and slightly modified for reasons of anonymity (nsubjects = 100; nvisits = 2 million). You can also work with your own data, which should have at least three columns called panelist_id, url and timestamp (in POSIXct format). Our toy data additionally have a variable called wave indicating before which of four fictional survey waves the data was collected, and device indicating whether a visit happened on desktop or mobile. Second, for some of the examples, you also need a survey data set (entirely made up), stored in the same OSF repository.\nR. In R, we can download the data sets directly like this:\n\n\n# load the test browsing data\nfilename &lt;- \"data/toydt_browsing.rda\"\ndownload.file(url = \"https://osf.io/download/wjd7g/\", destfile = filename)\nload(filename)\n\n# load the test survey data\nfilename &lt;- \"data/toydt_survey.rda\"\ndownload.file(url = \"https://osf.io/download/jyfru/\", destfile = filename)\nload(filename)\n\nrm(filename)\n\nRename the data sets to distinguish the object we work with in tidy R and with webtrackR.\n\n# \"dt\" is the browsing data we work with tidy R\ndt &lt;- toy_browsing\n\n# \"wt\" is the browsing data we work with webtrackR\nwt &lt;- as.wt_dt(toy_browsing)\n\nrm(toy_browsing)\n\nSQL. If you work with SQL, you can download the data sets with the links provided above, export them as csv files and re-import it as a table into your SQL environment.\n\n\n\nFor those less acquainted with computational methods, we begin with some basic examples so that you can familiarize yourself with browsing data. You will still need to install the packages and load the data as described above.\nFirst, let’s get a feeling for the data: How many participants are in the data?\n\ndt_warmup &lt;- dt\ndt_warmup %&gt;% distinct(panelist_id) %&gt;% nrow()\n\n[1] 100\n\n\nWhat is the number of visits per participant?\n\ndt_warmup %&gt;% group_by(panelist_id) %&gt;%\n  summarize(n_visits = n())\n\n# A tibble: 100 × 2\n   panelist_id n_visits\n   &lt;chr&gt;          &lt;int&gt;\n 1 0FcfTn9VDG     18075\n 2 1amvqVlZOT      9083\n 3 1c4XiaDGYx     14470\n 4 1i4XlNVP5E     58650\n 5 1wMa7rDJHE     47173\n 6 2na1X4OOYs     19852\n 7 32Apox0I46    111931\n 8 3u7JEyZBdP     12059\n 9 4HVpSqa344     60075\n10 5ptK95BQBY      3982\n# ℹ 90 more rows\n\n\nHow many of the visits happened on mobile and on desktop?\n\ntable(dt_warmup$device)\n\n\ndesktop  mobile \n1453627  618762 \n\n\nWhat is the time range of the data?\n\ndt_warmup %&gt;% summarize(\n  earliest = min(as.Date(timestamp)),\n  latest = max(as.Date(timestamp)))\n\n    earliest     latest\n1 2019-03-31 2019-07-31\n\n\nWhat are top ten domains in the data?\n\ndt_warmup &lt;- dt_warmup %&gt;% \n  mutate(domain = adaR::ada_get_domain(url))\n  \ndt_warmup %&gt;% group_by(domain) %&gt;%\n  summarize(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;% \n  head(10)\n\n# A tibble: 10 × 2\n   domain             n\n   &lt;chr&gt;          &lt;int&gt;\n 1 google.com    361960\n 2 facebook.com  133480\n 3 yahoo.com     112354\n 4 amazon.com     72593\n 5 youtube.com    39293\n 6 live.com       28393\n 7 mlb.com        27072\n 8 instagram.com  26533\n 9 bing.com       22646\n10 reddit.com     21956\n\n\nLet’s aggregate the data to the participant level, counting the number of visits to facebook.com of each participant, and join the survey data to the browsing data. Finally, let’s plot the number of visits to Facebook against participant age. Note that since the age variable was created randomly, we do not see any correlation here.\n\n# summarize number of Facebook visits per person\ndt_warmup_agg &lt;- dt_warmup %&gt;% \n  mutate(facebook = ifelse(domain == \"facebook.com\", 1, 0)) %&gt;%\n  group_by(panelist_id) %&gt;%\n  summarize(n_facebook = sum(facebook, na.rm = T))\n\n# join the survey data to the browsing data.\ndt_warmup_agg &lt;- dt_warmup_agg %&gt;% left_join(., toy_survey, by = \"panelist_id\")\n\n# plot the relation of Facebook visits and age\nplot(dt_warmup_agg$age, dt_warmup_agg$n_facebook)"
  },
  {
    "objectID": "index.html#computational-environment",
    "href": "index.html#computational-environment",
    "title": "Analyis of Web Browsing Data",
    "section": "",
    "text": "R. To run the R code, you need to install and load the following packages.\n\n# install.packages(\"tidyverse\")\n# install.packages(\"adaR\")\n## install.packages(\"devtools\")\n# devtools::install_github(\"schochastics/webtrackR\")\n# install.packages(\"httr2\")\n# install.packages(\"rvest\")\n\nlibrary(tidyverse)\nlibrary(adaR)\nlibrary(webtrackR)\nlibrary(httr2)\nlibrary(rvest)\n\nThe computational environment in which all the below R code was run is the following:\n\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Amsterdam\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rvest_1.0.3          httr2_0.2.3          webtrackR_0.1.0.9000\n [4] adaR_0.3.0           lubridate_1.9.2      forcats_1.0.0       \n [7] stringr_1.5.1        dplyr_1.1.4          purrr_1.0.2         \n[10] readr_2.1.4          tidyr_1.3.0          tibble_3.2.1        \n[13] ggplot2_3.4.4        tidyverse_2.0.0     \n\n\nSQL. All SQL code was run on AWS Athena, but should work with any SQL platform that understands the Presto dialect."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Analyis of Web Browsing Data",
    "section": "",
    "text": "To run the code, you need, first, a toy browsing data set from our OSF repository. It is based on a real data set collected from a US sample in 2019, but reduced to fewer participants and slightly modified for reasons of anonymity (nsubjects = 100; nvisits = 2 million). You can also work with your own data, which should have at least three columns called panelist_id, url and timestamp (in POSIXct format). Our toy data additionally have a variable called wave indicating before which of four fictional survey waves the data was collected, and device indicating whether a visit happened on desktop or mobile. Second, for some of the examples, you also need a survey data set (entirely made up), stored in the same OSF repository.\nR. In R, we can download the data sets directly like this:\n\n\n# load the test browsing data\nfilename &lt;- \"data/toydt_browsing.rda\"\ndownload.file(url = \"https://osf.io/download/wjd7g/\", destfile = filename)\nload(filename)\n\n# load the test survey data\nfilename &lt;- \"data/toydt_survey.rda\"\ndownload.file(url = \"https://osf.io/download/jyfru/\", destfile = filename)\nload(filename)\n\nrm(filename)\n\nRename the data sets to distinguish the object we work with in tidy R and with webtrackR.\n\n# \"dt\" is the browsing data we work with tidy R\ndt &lt;- toy_browsing\n\n# \"wt\" is the browsing data we work with webtrackR\nwt &lt;- as.wt_dt(toy_browsing)\n\nrm(toy_browsing)\n\nSQL. If you work with SQL, you can download the data sets with the links provided above, export them as csv files and re-import it as a table into your SQL environment."
  },
  {
    "objectID": "index.html#warmup",
    "href": "index.html#warmup",
    "title": "Analyis of Web Browsing Data",
    "section": "",
    "text": "For those less acquainted with computational methods, we begin with some basic examples so that you can familiarize yourself with browsing data. You will still need to install the packages and load the data as described above.\nFirst, let’s get a feeling for the data: How many participants are in the data?\n\ndt_warmup &lt;- dt\ndt_warmup %&gt;% distinct(panelist_id) %&gt;% nrow()\n\n[1] 100\n\n\nWhat is the number of visits per participant?\n\ndt_warmup %&gt;% group_by(panelist_id) %&gt;%\n  summarize(n_visits = n())\n\n# A tibble: 100 × 2\n   panelist_id n_visits\n   &lt;chr&gt;          &lt;int&gt;\n 1 0FcfTn9VDG     18075\n 2 1amvqVlZOT      9083\n 3 1c4XiaDGYx     14470\n 4 1i4XlNVP5E     58650\n 5 1wMa7rDJHE     47173\n 6 2na1X4OOYs     19852\n 7 32Apox0I46    111931\n 8 3u7JEyZBdP     12059\n 9 4HVpSqa344     60075\n10 5ptK95BQBY      3982\n# ℹ 90 more rows\n\n\nHow many of the visits happened on mobile and on desktop?\n\ntable(dt_warmup$device)\n\n\ndesktop  mobile \n1453627  618762 \n\n\nWhat is the time range of the data?\n\ndt_warmup %&gt;% summarize(\n  earliest = min(as.Date(timestamp)),\n  latest = max(as.Date(timestamp)))\n\n    earliest     latest\n1 2019-03-31 2019-07-31\n\n\nWhat are top ten domains in the data?\n\ndt_warmup &lt;- dt_warmup %&gt;% \n  mutate(domain = adaR::ada_get_domain(url))\n  \ndt_warmup %&gt;% group_by(domain) %&gt;%\n  summarize(n = n()) %&gt;%\n  arrange(desc(n)) %&gt;% \n  head(10)\n\n# A tibble: 10 × 2\n   domain             n\n   &lt;chr&gt;          &lt;int&gt;\n 1 google.com    361960\n 2 facebook.com  133480\n 3 yahoo.com     112354\n 4 amazon.com     72593\n 5 youtube.com    39293\n 6 live.com       28393\n 7 mlb.com        27072\n 8 instagram.com  26533\n 9 bing.com       22646\n10 reddit.com     21956\n\n\nLet’s aggregate the data to the participant level, counting the number of visits to facebook.com of each participant, and join the survey data to the browsing data. Finally, let’s plot the number of visits to Facebook against participant age. Note that since the age variable was created randomly, we do not see any correlation here.\n\n# summarize number of Facebook visits per person\ndt_warmup_agg &lt;- dt_warmup %&gt;% \n  mutate(facebook = ifelse(domain == \"facebook.com\", 1, 0)) %&gt;%\n  group_by(panelist_id) %&gt;%\n  summarize(n_facebook = sum(facebook, na.rm = T))\n\n# join the survey data to the browsing data.\ndt_warmup_agg &lt;- dt_warmup_agg %&gt;% left_join(., toy_survey, by = \"panelist_id\")\n\n# plot the relation of Facebook visits and age\nplot(dt_warmup_agg$age, dt_warmup_agg$n_facebook)"
  },
  {
    "objectID": "index.html#parsing-urls",
    "href": "index.html#parsing-urls",
    "title": "Analyis of Web Browsing Data",
    "section": "Parsing URLs",
    "text": "Parsing URLs\n\nExtracting hosts and domains\ntidy R. As discussed in the paper’s SM A, the most flexible package to extract domains and hosts is adaR (Schoch and Chan 2023). Often, URLs in web browsing data miss the protocol (“https://”), in which case you need to add it before extracting domain or host. Let’s extract hosts and domains and print a random set of URLs along with extracted host and domain:\n\n# add a protocol if necessary\ndt &lt;- dt %&gt;% \n  mutate(protocol = adaR::ada_get_protocol(url)) %&gt;%\n  mutate(url = ifelse(is.na(protocol), paste0(\"https://\", url), url))\n    \n# extract the host \ndt &lt;- dt %&gt;% mutate(host = adaR::ada_get_host(url))\n\n# extract the domain \ndt &lt;- dt %&gt;% mutate(domain = adaR::ada_get_domain(url))\n\n# inspect some results\ndt %&gt;% select(url, host, domain) %&gt;% sample_n(5)\n\n                                                                                                                        url\n1                                                                                             https://www.google.com/search\n2                                                      https://www.mlb.com/tv/g566223/ve31e0f1a-986b-492a-afe7-614e8153b47f\n3 https://www.lordandtaylor.com/shiseido-your-gift-with-any-75-shiseido-purchase-essential-energy-set/product/0500089152901\n4                                                                        https://www.instagram.com/stories/lauriehernandez/\n5                                      https://docs.google.com/document/d/1HjM902cuQdB1NEeg9BVFfplGP5amgln5cLiyQ5tlppY/edit\n                   host            domain\n1        www.google.com        google.com\n2           www.mlb.com           mlb.com\n3 www.lordandtaylor.com lordandtaylor.com\n4     www.instagram.com     instagram.com\n5       docs.google.com        google.com\n\n\nwebtrackR The webtrackR package wraps functions by adaR and takes care of missing protocols:\n\n# extract the host \nwt &lt;- extract_host(wt)\n\n# extract the domain \nwt &lt;- extract_domain(wt)\n\n# inspect some results\nwt %&gt;% select(url, host, domain) %&gt;% sample_n(5)\n\n                                                       url\n1 https://opinionsresearch.com/survey/selfserve/58e/a25454\n2                    https://www.instagram.com/Roxxsaurus/\n3             https://www.beasurveytaker.com/RouterSurvey/\n4          https://www.net-research.com/e/425249/index.php\n5                 https://www.newegg.com/p/N82E16826814041\n                    host               domain\n1   opinionsresearch.com opinionsresearch.com\n2      www.instagram.com        instagram.com\n3 www.beasurveytaker.com   beasurveytaker.com\n4   www.net-research.com     net-research.com\n5         www.newegg.com           newegg.com\n\n\nSQL. Presto SQL has an inbuilt function to extract the host:\n\nSELECT *, URL_EXTRACT_HOST(url)) AS host\nFROM dt\n\nFor domain extraction, it has no inbuilt function. One approach could be to upload the list of public suffixes as an SQL table, then to split up the URL host at every dot into parts; then to put the parts together incrementally, starting from the last part; finally match the resulting combinations to suffixes from the Public Suffix List. This strategy is explained in detail in this blog post. However, it is quite cumbersome, and the simpler approach is to download the list of unique hosts, extract the domains with the packages described above, and upload the table into the SQL database.\n\n\nExtracting paths and queries\ntidy R. Let’s extract paths and query and print a random set of URLs along with extracted paths and query:\n\n# extract the path \ndt &lt;- dt %&gt;% mutate(path = adaR::ada_get_pathname(url))\n\n# extract the query \ndt &lt;- dt %&gt;% mutate(query = adaR::ada_get_search(url))\n\n# inspect some results\ndt %&gt;% select(url, path, query) %&gt;% sample_n(5)\n\n                                                                                     url\n1                                                 https://account.microsoft.com/rewards/\n2                                                   https://www.amazon.com/dp/1683370759\n3                                 https://kelly--c.visualforce.com/apex/CandExpJobSearch\n4 https://jezebel.com/prison-might-be-pretty-embarrassing-for-the-varsity-blu-1834442677\n5                                http://adserver.entertainow.com/promo/LHCwn3f9oCbAlnHY/\n                                                                 path query\n1                                                           /rewards/      \n2                                                      /dp/1683370759      \n3                                              /apex/CandExpJobSearch      \n4 /prison-might-be-pretty-embarrassing-for-the-varsity-blu-1834442677      \n5                                            /promo/LHCwn3f9oCbAlnHY/      \n\n\nwebtrackR. The package has a function for extracting the path, not yet one for extracting the query:\n\n# extract the path \nwt &lt;- extract_path(wt)\n\n# inspect some results\nwt %&gt;% select(url, path) %&gt;% sample_n(5)\n\n                                                                                 url\n1 https://www.offroadexpo.com/video-scottsdale-off-road-expo-presented-by-nitto-2017\n2                                                          https://www.facebook.com/\n3                                           https://www.instagram.com/candancesmith/\n4                                             https://www.creditkarma.com/auth/logon\n5                                   https://accounts.google.com/signin/v2/identifier\n                                                     path\n1 /video-scottsdale-off-road-expo-presented-by-nitto-2017\n2                                                       /\n3                                         /candancesmith/\n4                                             /auth/logon\n5                                   /signin/v2/identifier\n\n\nAdditionally, webtrackR allows users to parse the path if it contains a human-readable string. As described in Section 4.3 of the paper, this information can be used for further classification. Since URL paths contain many non-human-readable strings, the package allows to specify whether to only keep strings consisting of letters, or only those consisting of English words:\n\n# parse the paths into human-readable text\nwt &lt;- parse_path(wt, varname = \"url\", keep = \"letters_only\")\n\n# inspect some results\nwt %&gt;% select(url, path, path_split) %&gt;% sample_n(5)\n\n                                                                                         url\n1       https://docs.google.com/document/d/10zjeUc3kJmgr49lmjnbZYBwfcKQ83sYVaS8SN6twFpg/edit\n2                                                          https://mail.google.com/mail/u/0/\n3 https://photos.google.com/search/May 25/photo/AF1QipMbW-Zj7WErhF4WVZP87Y0VrVFfrX3F-zQRSKT6\n4        https://www.facebook.com/kentuckianahumor/photos/a.180682925896636/267987777166150/\n5                                                              https://www.google.com/search\n                                                               path\n1     /document/d/10zjeUc3kJmgr49lmjnbZYBwfcKQ83sYVaS8SN6twFpg/edit\n2                                                        /mail/u/0/\n3 /search/May 25/photo/AF1QipMbW-Zj7WErhF4WVZP87Y0VrVFfrX3F-zQRSKT6\n4       /kentuckianahumor/photos/a.180682925896636/267987777166150/\n5                                                           /search\n                 path_split\n1           document,d,edit\n2                    mail,u\n3              search,photo\n4 kentuckianahumor,photos,a\n5                    search\n\n\nSQL. Presto SQL has inbuilt-functions to extract path and query:\n\nSELECT *, URL_EXTRACT_PATH(url)) AS path, URL_EXTRACT_QUERY(url)) AS query \nFROM dt"
  },
  {
    "objectID": "index.html#defining-visit-duration",
    "href": "index.html#defining-visit-duration",
    "title": "Analyis of Web Browsing Data",
    "section": "Defining visit duration",
    "text": "Defining visit duration\ntidy R. The following code creates a timestamp-based duration variable, with a cutoff of 300 seconds and setting differences exceeding it to NA. Further, the visit before a device switch, as well as the last visit of a person, is set to NA.\n\n# Define cutoff and replacement\ncutoff &lt;- 300\nreplacement &lt;- NA\n\n# create \"next\" timestamp and \"next\" device\ndt &lt;- dt %&gt;%\n  group_by(panelist_id) %&gt;%\n  mutate(timestamp_next = lead(timestamp, order_by = timestamp),\n         device_next = lead(device, order_by = timestamp)) \n\n# create timestamp-based difference\ndt &lt;- dt %&gt;% \n  ungroup() %&gt;%\n  arrange(panelist_id, timestamp) %&gt;% \n  # compute raw time difference between timestamps\n  mutate(timediff = timestamp_next - timestamp) %&gt;%\n  # set time difference to NA at device switch\n  mutate(timediff = ifelse(device != device_next, NA, timediff)) %&gt;%\n  # set duration to time difference unless exceeding cutoff\n  mutate(duration = ifelse(timediff &gt; cutoff, NA, timediff)) %&gt;%\n  select(-c(timestamp_next, device_next, timediff))\n\nwebtrackR. The package wraps this logic in a function. It allows to set the cutoff value, the replacement value, and whether the duration before a device switch should be set to NA.\n\nwt &lt;- add_duration(\n  wt, cutoff = 300, replace_by = NA, \n  device_switch_na = F, device_var = \"device\")\n\nSQL. In Presto SQL, the same can be done with the following:\n\nSELECT panelist_id, wave, timestamp, url, device,\n-- set duration to time difference unless exceeding cutoff, or before device switch\nCASE \nWHEN device != device_next THEN NULL\nWHEN date_diff('second', timestamp, timestamp_next) &gt; 300 THEN NULL\nELSE date_diff('second', timestamp, timestamp_next)\nEND AS duration\nFROM\n(SELECT *,\n-- create \"next\" timestamp and \"next\" device\nLEAD(timestamp) OVER(PARTITION BY panelist_id ORDER BY timestamp) AS timestamp_next,\nLEAD(device) OVER(PARTITION BY panelist_id ORDER BY timestamp) AS device_next\nFROM guide_testdata)"
  },
  {
    "objectID": "index.html#defining-referrals",
    "href": "index.html#defining-referrals",
    "title": "Analyis of Web Browsing Data",
    "section": "Defining referrals",
    "text": "Defining referrals\ntidy R. The below code examples implements the approach judged as optimal by Schmidt et al. (2023): A visit is defined as referred by a platform (e.g., Facebook) when it directly follows a visit to that platform and contains a certain pattern in its URL (e.g., “fbclid=”).\n\n# the following assumes you have extracted the domain already\ndt &lt;- dt %&gt;%\n  # get previous domain\n  group_by(panelist_id) %&gt;%\n  mutate(domain_previous = dplyr::lag(domain, order_by = timestamp)) %&gt;%\n  # flag referral\n  mutate(referral = ifelse(domain_previous == \"facebook.com\" & grepl(\"fbclid=\", url), 1, 0)) %&gt;%\n  select(-c(domain_previous))\n\nwebtrackR. The package implements the logic in a function that extracts the previous domain automatically and also allows getting referrals for multiple platforms/patterns:\n\nwt &lt;- add_referral(wt, platform_domains = \"facebook.com\", patterns = \"fbclid=\")\n\nSQL. The same approach would work as follows in Presto SQL, assuming the domain has been extracted:\n\nSELECT *, \nCASE \nWHEN regexp_like('fbclid=', url) AND domain_previous = 'facebook.com' THEN 1\nELSE 0 \nEND AS referral\nFROM\n(SELECT url, timestamp, \nlag(domain) OVER (PARTITION BY panelist_id ORDER BY timestamp) AS domain_previous\nFROM dt)"
  },
  {
    "objectID": "index.html#missing-visits",
    "href": "index.html#missing-visits",
    "title": "Analyis of Web Browsing Data",
    "section": "Missing visits",
    "text": "Missing visits\nR. For each participant, we can compute the proportion of the span of active days (i.e., the number of days between the first and last day of data for that person) out of the potential maximum of days (i.e., the difference between the first and last day of data collection). If this proportion is small, it could be an indication of missingness—of course, given certain theoretical assumptions. As the histogram below shows, there are indeed some subjects for which the proportion is small.\n\n# define the maximum span of days as difference between first and last day\nmax_days &lt;- as.numeric(max(as.Date(dt$timestamp)) - min(as.Date(dt$timestamp)))\n\n# summarize per person\nperson_summary &lt;- dt %&gt;%\n  mutate(date = as.Date(timestamp)) %&gt;%\n  group_by(panelist_id, date) %&gt;%\n  summarize(n_visits = n()) %&gt;%\n  group_by(panelist_id) %&gt;%\n  summarise(earliest = as.Date(min(date)),\n            latest = as.Date(max(date))) %&gt;%\n  mutate(span_days = as.numeric(latest - earliest) + 1) %&gt;%\n  mutate(proportion = span_days / max_days)\n\n# inspect the resulting proportion\nhist(person_summary$proportion, breaks = 100)\n\n\n\n\nSQL. The same can be achieved in Presto as follows:\n\n-- define the maximum span of days as difference between first and last day: 122\nSELECT MAX(DATE(timestamp)) - MIN(DATE(timestamp)) \nFROM dt\n\n-- summarize per person \nSELECT panelist_id, earliest, latest,\nDATE_DIFF('day', earliest, latest) AS span_days,\nCAST(DATE_DIFF('day', earliest, latest) AS REAL) / max_days AS proportion\nFROM\n(SELECT panelist_id,\nMIN(DATE(timestamp)) earliest, MAX(DATE(timestamp)) latest, 122 as max_days\nFROM\n(SELECT panelist_id, timestamp, COUNT() n_visits\nFROM dt\nWHERE wave = 1\nGROUP BY panelist_id, timestamp)\nGROUP BY panelist_id)"
  },
  {
    "objectID": "index.html#duplicated-visits",
    "href": "index.html#duplicated-visits",
    "title": "Analyis of Web Browsing Data",
    "section": "Duplicated visits",
    "text": "Duplicated visits\nAs we describe in the main paper, duplicates can be defined in several ways: as sequential visits to the same URL, after removing the URL’s fragment (Guess 2021); as visits to the same URL within the same day (Wojcieszak et al. 2022); or as sequential visits to the same URL that happen within one second. Alternatively, one can aggregate the duration of sequential visits to the same URL (Stier et al. 2022).\ntidy R. The four methods can be applied to the data the following way:\n\n# Guess 2021\ndt &lt;- dt %&gt;%\n  mutate(protocol = ada_get_protocol(url),\n         host = ada_get_host(url),\n         path = ada_get_pathname(url),\n         query = ada_get_search(url),\n         fragment = ada_get_hash(url))  %&gt;%\n  # construct URL without fragment\n  mutate(url_without_fragment = paste0(protocol, host, path, query)) %&gt;%\n  select(-c(protocol, fragment)) %&gt;%\n  arrange(panelist_id, timestamp) %&gt;%\n  group_by(panelist_id) %&gt;%\n  # previous URL without fragment\n  mutate(url_prev_without_fragment = lag(url_without_fragment, order_by = timestamp),\n         person_visit_rank = 1:n()) %&gt;%\n  ungroup() %&gt;%\n  # if the previous URL is the same, then duplicate\n  mutate(dupl_guess = ifelse(url_without_fragment == url_prev_without_fragment, 1, 0)) %&gt;%\n  # ... and the first visit of a person is 0, rather than NA \n  mutate(dupl_guess = ifelse(person_visit_rank == 1, 0, dupl_guess)) %&gt;%\n  select(-c(url_prev_without_fragment, url_without_fragment))\n\n# Wojcieszak et al 2022\ndt &lt;- dt %&gt;%\n  mutate(date = as.Date(timestamp)) %&gt;%\n  arrange(panelist_id, date, timestamp) %&gt;%\n  group_by(panelist_id, date, url) %&gt;%\n  # give unique number for URLs per date/person\n  mutate(rep_number = 1:n()) %&gt;%\n  ungroup() %&gt;%\n  # any URL that has been visited already by person that day is duplicate\n  mutate(dupl_wojc = ifelse(rep_number &gt; 1, 1, 0)) %&gt;%\n  select(-c(date, rep_number))\n\n# sequential visits to the same URL that happen within one second\ncutoff &lt;- 1 \ndt &lt;- dt %&gt;%\n  arrange(panelist_id, timestamp) %&gt;%\n  group_by(panelist_id) %&gt;%\n  mutate(timestamp_prev = lag(timestamp, order_by = timestamp),\n         url_prev = lag(url, order_by = timestamp),\n         person_visit_rank = 1:n()) %&gt;%\n  ungroup() %&gt;%\n  mutate(created_diff = difftime(timestamp, timestamp_prev)) %&gt;%\n  # if previous URL is the same and within one sec, then duplicate\n  mutate(dupl_onesec = ifelse((url == url_prev & created_diff &lt;= cutoff), 1, 0)) %&gt;%\n  mutate(dupl_onesec = ifelse(person_visit_rank == 1, 0, dupl_onesec)) %&gt;%\n  select(-c(timestamp_prev, url_prev, person_visit_rank, created_diff))\n\n# aggregate duration of sequential visits to the same URL\ndt_agged &lt;- dt %&gt;%\n  arrange(panelist_id, timestamp) %&gt;%\n  group_by(panelist_id) %&gt;%\n  mutate(url_group = cumsum(url != lag(url, def = first(url)))) %&gt;%\n  group_by(panelist_id, wave, url, url_group) %&gt;%\n  summarize(agg_duration = sum(duration, na.rm = T),\n            timestamp_first = min(timestamp, na.rm = T)) %&gt;%\n  ungroup()\n\n# inspect the resulting duplicate proportions\ndt %&gt;% \n  summarize(prop_dupl_guess = sum(dupl_guess, na.rm = T) / n(),\n            prop_dupl_wojc = sum(dupl_wojc, na.rm = T) / n(),\n            prop_dupl_onesec = sum(dupl_onesec, na.rm = T) / n())\n\n# A tibble: 1 × 3\n  prop_dupl_guess prop_dupl_wojc prop_dupl_onesec\n            &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1           0.395          0.662            0.139\n\n\nwebtrackR. The package implements both the logic of treating sequential visits to the same URL that happen within one second as duplicates. It also allows users aggregation of consecutive visits to the same URL.\n\n# flagging duplicates within one second\nwt_dedup &lt;- deduplicate(wt, method = \"flag\", within = 1)\n\n# dropping duplicates within one second\nwt_dedup &lt;- deduplicate(wt, method = \"drop\", within = 1)\n\n# aggregate duration of sequential visits to the same URL\nwt_dedup &lt;- deduplicate(wt, method = \"aggregate\", keep_nvisits = TRUE)\n\nSQL. Flagging/dropping duplicates defined as sequential visits within the same second, as well as aggregating the duration of sequential visits would work as follows in SQL:\n\n-- flagging duplicates within one second\nSELECT *,\nCASE \nWHEN timestamp_prev IS NULL THEN FALSE\nWHEN date_diff('second', timestamp_prev, timestamp) &lt;= 1 AND url = url_prev THEN TRUE\nELSE FALSE\nEND AS duplicate\nFROM\n(SELECT panelist_id,  \ntimestamp, LAG(timestamp) OVER (PARTITION BY panelist_id ORDER BY timestamp) AS timestamp_prev, \nurl, LEAD(url) OVER (PARTITION BY panelist_id ORDER BY timestamp) AS url_prev\nFROM dt)\n\n-- dropping duplicates within one second\nSELECT * FROM\n(SELECT *,\nCASE \nWHEN timestamp_prev IS NULL THEN FALSE\nWHEN date_diff('second', timestamp_prev, timestamp) &lt;= 1 AND url = url_prev THEN TRUE\nELSE FALSE\nEND AS duplicate\nFROM\n(SELECT panelist_id,  \ntimestamp, LAG(timestamp) OVER (PARTITION BY panelist_id ORDER BY timestamp) AS timestamp_prev, \nurl, LEAD(url) OVER (PARTITION BY panelist_id ORDER BY timestamp) AS url_prev\nFROM dt))\nWHERE duplicate = TRUE\n\n-- aggregate duration of sequential visits to the same URL (assuming duration is extracted)\nSELECT \nsum(duration) AS agg_duration, MIN(timestamp) AS timestamp_first\nFROM\n(SELECT *, \nSUM(new_url) OVER (PARTITION BY panelist_id ORDER BY timestamp) url_group\nFROM\n(SELECT *\nCASE \nWHEN url_no_query = LAG(url_no_query) OVER (PARTITION BY panelist_id ORDER BY timestamp) THEN 0\nELSE 1 \nEND AS new_url\nFROM dt \nORDER BY panelist_id, timestamp))\nGROUP BY panelist_id, wave, url, url_group"
  },
  {
    "objectID": "index.html#incentivized-visits",
    "href": "index.html#incentivized-visits",
    "title": "Analyis of Web Browsing Data",
    "section": "Incentivized visits",
    "text": "Incentivized visits\nR. To identify survey visits, you can use the list of hosts identified by Clemm von Hohenberg, Ventura, et al. (2024), which includes three types of entries: (1) hosts containing the word “survey”; (2) hosts matching the address of questionnaires platforms listed in a report by Bevec and Vehovar (2021); (3) those among the most frequented hosts identified as survey sites. Hence, this list is quite specific to the data analyzed in Clemm von Hohenberg, Ventura, et al. (2024). For example, in your data set that there may be other URL hosts containing the word “survey” not on our list. Thus, a more rigorous approach would be reproduce the approach on your own data set.\n\n\n# download the list from OSF\nfilename &lt;- \"data/survey-click2earn-hosts.csv\"\ndownload.file(url = \"https://osf.io/download/dw6cq/\", destfile = filename)\nsurvey_click2earn_hosts &lt;- read.csv(filename)\nrm(filename)\n\n# match the list to the browsing data (assumes that host is already extracted)\ndt &lt;- dt %&gt;%\n  left_join(., survey_click2earn_hosts, by = c(\"host\" = \"url_host\")) %&gt;%\n  mutate(survey_flag = ifelse(!is.na(method), T, F))\n\nSQL. This would translate to Presto the following way, assuming the list of survey hosts is uploaded as a table:\n\n-- join tracking data to list of survey hosts and flag visits\nSELECT \nd.url, d.url_host, s.method,\nCASE \nWHEN s.method IS NOT NULL THEN TRUE \nELSE FALSE\nEND AS survey_flag\nFROM\n(SELECT *, URL_EXTRACT_HOST(url) as url_host\nFROM dat) d\nLEFT JOIN survey_click2earn_hosts s ON d.url_host = s.url_host"
  },
  {
    "objectID": "index.html#classifying-by-domains-and-hosts",
    "href": "index.html#classifying-by-domains-and-hosts",
    "title": "Analyis of Web Browsing Data",
    "section": "Classifying by domains and hosts",
    "text": "Classifying by domains and hosts\nR. To exemplify, we match the list of news sites by Wojcieszak et al. (2023) to the browsing data. Since not all of the news sites consist of a domain, but some also have a path (e.g., “yahoo.com/news”), we first need to split them up in two:\n\nurl &lt;- \"https://raw.githubusercontent.com/ercexpo/us-news-domains/main/us-news-domains-v2.0.0.csv\"\nnews_list &lt;- read.csv(url)\n\n### Check which ones of our news sites are defined by domain only, which more than domain\nnews_list &lt;- news_list %&gt;%\n  rename(\"domain_old\" = domain) %&gt;%\n  mutate(https_domain = paste0(\"https://\", domain_old)) %&gt;%\n  mutate(domain = adaR::ada_get_domain(https_domain)) %&gt;%\n  select(domain, domain_old) %&gt;%\n  mutate(domain_match = ifelse(domain == domain_old, T, F))\n\n### News sites defined by domain\nnews_domains &lt;- news_list %&gt;% \n  filter(domain_match == T) %&gt;%\n  select(domain) %&gt;% \n  mutate(news = 1) %&gt;% \n  filter(!duplicated(domain))\n\n### News sites defined by more than domain (e.g. \"yahoo.com/new\")\n### Change these to a string to be matched with regex\nnews_other &lt;- news_list %&gt;% \n  filter(domain_match == F) %&gt;%\n  pull(domain_old) %&gt;% paste(., collapse = \"|\")\n\nThen, we match both the list of domains, and the list of domains with paths to the data:\n\ndt &lt;- dt %&gt;% \n  left_join(., news_domains, by = \"domain\") %&gt;%\n  mutate(news = ifelse(grepl(news_other, url), 1, news))\n\nThe logic is the same in webtrackR:\n\nwt &lt;- wt %&gt;% \n  left_join(., news_domains, by = \"domain\") %&gt;%\n  mutate(news = ifelse(grepl(news_other, url), 1, news))"
  },
  {
    "objectID": "index.html#classifying-by-web-site-titles-or-paths",
    "href": "index.html#classifying-by-web-site-titles-or-paths",
    "title": "Analyis of Web Browsing Data",
    "section": "Classifying by web site titles or paths",
    "text": "Classifying by web site titles or paths\ntidy R. The following R code lets you get titles for URLs with the help of the rvest package. Note that for a large number of URLs, this process can take a while. We therefore test the code with only a small selection of ten URLs. The results show that for a lot of cases, the title cannot be retrieved any more, which is unsurprising given the data collection happened in 2019.\n\n# usually, you would get unique set of URls to speed up the process:\nurls_unique &lt;- data.frame(url = unique(dt$url)) \n\n# for now, we just get a random sample of ten:\nurls_sample &lt;- slice_sample(urls_unique, n = 10)\n\n# for each URL, get the title (and set to NA if irretrievable)\nurls_sample &lt;- urls_sample %&gt;%\n  mutate(title = mapply(function(x) {\n    return(\n      tryCatch({\n        print(x)\n        html_text(html_node(read_html(x), \"head title\"))\n      }, error = function(e) NA))}, url))\n\n[1] \"https://autismup.org/classes\"\n[1] \"https://store.roosterteeth.com/products/rwby-ruby-sketch-t-shirt\"\n[1] \"https://www.socialscour.com/ss3/search/web\"\n[1] \"https://en.wikipedia.org/wiki/The_Perks_of_Being_a_Wallflower_(film)\"\n[1] \"https://www.celebjihad.com/kelly-gale/kelly-gale-nude-and-sexy-photos-compilation\"\n[1] \"https://worker.mturk.com/projects/3VKCYEEK5GMTPFGJS9C31OUU5FB1GV/tasks/3BAKUKE49HYJ95RMQEW5PANEBF0R19\"\n[1] \"https://www.swagbucks.com/watch/video/309595317/it-helps-to-have-dads-advice-when-you-win-a-golden-globe-award\"\n[1] \"https://www.amazon.com/gp/offer-listing/B009SNP2W6/SubscriptionId=AKIAJ7T5BOVUVRD2EFYQ&tag=camelproducts-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=B009SNP2W6\"\n[1] \"https://old.reddit.com/r/TalesFromTheFrontDesk/comments/bhwubj/guest_was_angry_at_me_because_of_the_security/\"\n[1] \"https://www.instagram.com/tidwell5451/channel/\"\n\n# had we done this for all URLs, we could join back to data\n# dt &lt;- dt %&gt;%\n#   left_join(., urls_unique, by = \"url\")\n\n# inspect some results\nurls_sample %&gt;% head()\n\n                                                                                                    url\n1                                                                          https://autismup.org/classes\n2                                      https://store.roosterteeth.com/products/rwby-ruby-sketch-t-shirt\n3                                                            https://www.socialscour.com/ss3/search/web\n4                                  https://en.wikipedia.org/wiki/The_Perks_of_Being_a_Wallflower_(film)\n5                     https://www.celebjihad.com/kelly-gale/kelly-gale-nude-and-sexy-photos-compilation\n6 https://worker.mturk.com/projects/3VKCYEEK5GMTPFGJS9C31OUU5FB1GV/tasks/3BAKUKE49HYJ95RMQEW5PANEBF0R19\n                                               title\n1                                               &lt;NA&gt;\n2                                               &lt;NA&gt;\n3                                               &lt;NA&gt;\n4 The Perks of Being a Wallflower (film) - Wikipedia\n5        Kelly Gale Nude And Sexy Photos Compilation\n6                                     Amazon Sign-In\n\n\nwebtrackR. The package wraps the above logic in a function and also allows to set the accepted language of the request. Note that this language setting does not overcome the fact that being in a different country will give you different results.\n\n# get a random sample of ten rows:\nwt_sample &lt;- slice_sample(wt, n = 10)\n\n# call the function\nwt_sample &lt;- add_title(wt_sample, lang = \"en\")\n\n# inspect some results\nwt_sample %&gt;% select(url, title) %&gt;% head()"
  },
  {
    "objectID": "index.html#visits--vs-time-based-exposure",
    "href": "index.html#visits--vs-time-based-exposure",
    "title": "Analyis of Web Browsing Data",
    "section": "Visits- vs time-based exposure",
    "text": "Visits- vs time-based exposure\nAs we have summarized both duration and visits, we can do both visits-based and time-based modelling. For example, two regressions of political knowledge measured at the last wave (polknow_w4) on sociodemographics, total browsing and news exposure, once in terms of visits and once in terms of time would look like this:\n\n# visits-based model\nlm(polknow_w4 ~ age + sex + edu + news_n + total_n, data = dt_person) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = polknow_w4 ~ age + sex + edu + news_n + total_n, \n    data = dt_person)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48150 -0.80613  0.02891  0.65293  2.34308 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.655e+00  3.536e-01  10.337  &lt; 2e-16 ***\nage          2.323e-03  5.501e-03   0.422  0.67382    \nsexmale     -2.615e-01  2.106e-01  -1.242  0.21748    \nedulow      -3.034e-01  2.735e-01  -1.109  0.27016    \nedumedium   -4.574e-01  2.442e-01  -1.873  0.06423 .  \nnews_n      -2.560e-04  8.690e-05  -2.946  0.00407 ** \ntotal_n     -2.377e-05  5.571e-06  -4.266 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.031 on 93 degrees of freedom\nMultiple R-squared:  0.3258,    Adjusted R-squared:  0.2823 \nF-statistic: 7.491 on 6 and 93 DF,  p-value: 1.456e-06\n\n\n\n# time-based model\nlm(polknow_w4 ~ age + sex + edu + news_sec + total_sec, data = dt_person) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = polknow_w4 ~ age + sex + edu + news_sec + total_sec, \n    data = dt_person)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.18260 -0.62362  0.04638  0.63464  2.27611 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.663e+00  3.390e-01  10.804  &lt; 2e-16 ***\nage          1.549e-03  5.293e-03   0.293   0.7705    \nsexmale     -2.644e-01  2.030e-01  -1.303   0.1959    \nedulow      -2.382e-01  2.618e-01  -0.910   0.3652    \nedumedium   -4.926e-01  2.340e-01  -2.105   0.0380 *  \nnews_sec    -1.767e-05  4.057e-06  -4.356 3.41e-05 ***\ntotal_sec   -5.232e-07  2.087e-07  -2.507   0.0139 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9931 on 93 degrees of freedom\nMultiple R-squared:  0.375, Adjusted R-squared:  0.3347 \nF-statistic: 9.301 on 6 and 93 DF,  p-value: 5.591e-08"
  },
  {
    "objectID": "index.html#accounting-for-skewness",
    "href": "index.html#accounting-for-skewness",
    "title": "Analyis of Web Browsing Data",
    "section": "Accounting for skewness",
    "text": "Accounting for skewness\nIf we think that the skew of the exposure variable needs to be accounted for by log-transformation, we could apply this transformation before running the model. Specifically, we take the natural log after adding 1 (so that a value of zero does not become undefined).\n\ndt_person &lt;- dt_person %&gt;%\n  mutate(across(c(news_n, total_n, news_sec, total_sec), ~ log(. + 1), \n         .names = \"{.col}_log\"))\n\n\n# log-transformed exposure (visits)\nlm(polknow_w4 ~ age + sex + edu + news_n_log + total_n_log, data = dt_person) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = polknow_w4 ~ age + sex + edu + news_n_log + total_n_log, \n    data = dt_person)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.85944 -0.56743  0.04893  0.48329  2.28713 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.077361   0.833504   6.092 2.50e-08 ***\nage          0.001496   0.004290   0.349    0.728    \nsexmale     -0.166664   0.164114  -1.016    0.312    \nedulow      -0.081956   0.213298  -0.384    0.702    \nedumedium   -0.225455   0.191448  -1.178    0.242    \nnews_n_log  -0.391735   0.054315  -7.212 1.44e-10 ***\ntotal_n_log -0.048755   0.101509  -0.480    0.632    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8046 on 93 degrees of freedom\nMultiple R-squared:  0.5897,    Adjusted R-squared:  0.5633 \nF-statistic: 22.28 on 6 and 93 DF,  p-value: 4.191e-16\n\n\n\n# log-transformed exposure (duration)\nlm(polknow_w4 ~ age + sex + edu + news_sec_log + total_sec_log, data = dt_person) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = polknow_w4 ~ age + sex + edu + news_sec_log + total_sec_log, \n    data = dt_person)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83086 -0.59240  0.03624  0.51253  2.33847 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    5.8064671  1.1611419   5.001 2.68e-06 ***\nage            0.0009637  0.0044266   0.218    0.828    \nsexmale       -0.0606849  0.1695799  -0.358    0.721    \nedulow        -0.1268052  0.2174683  -0.583    0.561    \nedumedium     -0.2846867  0.1967174  -1.447    0.151    \nnews_sec_log  -0.2799361  0.0418323  -6.692 1.64e-09 ***\ntotal_sec_log -0.0614377  0.1062990  -0.578    0.565    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8284 on 93 degrees of freedom\nMultiple R-squared:  0.5651,    Adjusted R-squared:  0.5371 \nF-statistic: 20.14 on 6 and 93 DF,  p-value: 5.808e-15"
  },
  {
    "objectID": "index.html#controlling-for-overall-browsing",
    "href": "index.html#controlling-for-overall-browsing",
    "title": "Analyis of Web Browsing Data",
    "section": "Controlling for overall browsing",
    "text": "Controlling for overall browsing\nWhether or not we need to control for overall browsing is a theoretical decision, and we can easily run the model with or without that control. Alternatively, we can also create a proportional measure of exposure:\n\ndt_person &lt;- dt_person %&gt;%\n  mutate(news_prop = news_n / total_n)\n\n\n# no control for overall browsing\nlm(polknow_w4 ~ age + sex + edu + news_n, data = dt_person) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = polknow_w4 ~ age + sex + edu + news_n, data = dt_person)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.63183 -0.81035 -0.01082  0.78559  2.79764 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.053e+00  3.526e-01   8.658  1.3e-13 ***\nage          3.639e-03  5.974e-03   0.609 0.543932    \nsexmale     -2.005e-01  2.285e-01  -0.877 0.382472    \nedulow      -1.144e-01  2.935e-01  -0.390 0.697697    \nedumedium   -4.211e-01  2.655e-01  -1.586 0.116059    \nnews_n      -3.637e-04  9.043e-05  -4.021 0.000117 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.122 on 94 degrees of freedom\nMultiple R-squared:  0.1939,    Adjusted R-squared:  0.151 \nF-statistic: 4.522 on 5 and 94 DF,  p-value: 0.0009733\n\n\n\n# control for overall browsing\nlm(polknow_w4 ~ age + sex + edu + news_n + total_n, data = dt_person) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = polknow_w4 ~ age + sex + edu + news_n + total_n, \n    data = dt_person)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48150 -0.80613  0.02891  0.65293  2.34308 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.655e+00  3.536e-01  10.337  &lt; 2e-16 ***\nage          2.323e-03  5.501e-03   0.422  0.67382    \nsexmale     -2.615e-01  2.106e-01  -1.242  0.21748    \nedulow      -3.034e-01  2.735e-01  -1.109  0.27016    \nedumedium   -4.574e-01  2.442e-01  -1.873  0.06423 .  \nnews_n      -2.560e-04  8.690e-05  -2.946  0.00407 ** \ntotal_n     -2.377e-05  5.571e-06  -4.266 4.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.031 on 93 degrees of freedom\nMultiple R-squared:  0.3258,    Adjusted R-squared:  0.2823 \nF-statistic: 7.491 on 6 and 93 DF,  p-value: 1.456e-06\n\n\n\n# proportional measure\nlm(polknow_w4 ~ age + sex + edu + news_prop, data = dt_person) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = polknow_w4 ~ age + sex + edu + news_prop, data = dt_person)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.12084 -0.80752  0.00932  0.84083  2.85510 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.952685   0.356308   8.287 7.96e-13 ***\nage          0.004688   0.006059   0.774 0.440992    \nsexmale     -0.178137   0.231886  -0.768 0.444289    \nedulow      -0.045496   0.297556  -0.153 0.878806    \nedumedium   -0.382742   0.270193  -1.417 0.159920    \nnews_prop   -8.331447   2.259622  -3.687 0.000379 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.135 on 94 degrees of freedom\nMultiple R-squared:  0.1746,    Adjusted R-squared:  0.1307 \nF-statistic: 3.977 on 5 and 94 DF,  p-value: 0.002573\n\n\n\n\n–&gt;"
  }
]